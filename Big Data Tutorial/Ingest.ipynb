{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://camo.githubusercontent.com/290ee18049382f170e5f63956a389da081a8c16f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f636f6d2e726176656e7061636b2e636d732f70616765732f6a702d6d6f7267616e2d6269672d646174612d61692d35343637413139302e6a7067\"  style=\"width: 600px; height: 163px\">\n</div>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "%fs unmount /mnt/deepfin3"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "%fs ls /mnt/"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport os\nfrom bs4 import BeautifulSoup\nfrom azure.storage.blob import BlockBlobService",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "wasb_account_key = 'TODO'\nwasb_account_name = 'TODO'\n\n# Create the BlockBlockService that is used to call the Blob service for the storage account\nblock_blob_service = BlockBlobService(account_name=wasb_account_name, account_key=wasb_account_key)\n\n# Create a container called 'quickstartblobs'.\ncontainer_name ='deepfin3'\nif block_blob_service.exists(container_name) is False:\n  block_blob_service.create_container(container_name)      ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def scrape_find_zips(url):\n    \"\"\" scrape the given url for any zip files in href tags \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1',\n    }\n\n    response = requests.get(url, headers=headers)\n    #print(response.status_code)\n    #print(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    all_tags = soup.find_all('a', href=True)\n    tag_dict = {}\n    for tag in all_tags:\n      if '.zip' in tag['href']:\n          tag_dict[tag.get_text()] = tag['href']\n    return tag_dict\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "url = \"https://www.asxhistoricaldata.com/archive/\"\n\ntag_dict = scrape_find_zips(url)\n\nprint(tag_dict.values())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.storage.file import ContentSettings\n\ndef scrape_downloader(urls, dest):\n    \"\"\" download all files listed in urls {} dict \n    Input:\n        url = base url\n        urls= target paths to append to base url\n        dest= local path to store files\n    \"\"\"\n    for k_name, v_path in urls.items():\n        src = v_path        \n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1',\n        }\n        r = requests.get(src, headers=headers)  \n        if r is not None:\n          block_blob_service.create_blob_from_bytes(dest, k_name + '.zip', r.content)            \n          print(src)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scrape_downloader(tag_dict, container_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "blobs = block_blob_service.list_blobs(container_name)\n\nfor blob in blobs:\n  print(blob.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import zipfile\nimport csv\nimport datetime\nfrom io import BytesIO, TextIOWrapper\nfrom pyspark.sql import Row\n\n\n#get all files in container\nf_zip = [ blob.name for blob in blobs]\n\n#Create empty lists\nnewlines = lines = header = all_lines = []\n\n#limit to single \nfor f in f_zip: #[:1]: limit\n    #only go through zip files in container \n    if f.endswith(\".zip\"):\n      f_bytes = block_blob_service.get_blob_to_bytes(container_name, f)\n      with zipfile.ZipFile(BytesIO(f_bytes.content)) as main_zip:  \n        for inner_file in main_zip.namelist():   \n            #print(inner_file)\n            #read inner zip file\n            zfiledata = BytesIO(main_zip.read(inner_file))\n            inner_csv = TextIOWrapper(zfiledata, encoding='utf8', newline='')            \n            reader = csv.reader(inner_csv, delimiter=',', dialect=csv.unix_dialect)\n            \n            for row, line in enumerate(reader):\n                #print(line)\n                # 'Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume'\n                lines.append(Row(Ticker=line[0], Date=datetime.datetime.strptime(line[1], '%Y%m%d') , High=float(line[2]), Low=float(line[3]), Close=float(line[4]), Volume=float(line[5])))\n            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = sqlContext.createDataFrame(sc.parallelize(lines))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.printSchema()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%sql drop table if exists asxhistoric;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.write.saveAsTable(\"asxhistoric\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pyspark.sql.functions import year\n\nfocus_df = ((df \n  .select(\"Date\", \"Close\", \"Ticker\") \n  .filter(\"Ticker = 'AAA'\")\n  .filter(year(\"Date\") == '1998')\n))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "focus_df.count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "display(focus_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%sql drop table if exists timeseries;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "focus_df.write.saveAsTable(\"timeseries\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pyspark.sql.functions import year\n\ndf_2016 = ((df \n  .filter(year(\"Date\") == '2016')\n))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%sql drop table if exists asxhistoric2016;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_2016.write.saveAsTable(\"asxhistoric2016\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%sql\nSELECT COUNT(*) FROM asxhistoric2016;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "name": "Ingest",
    "notebookId": 1521445780264667,
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}



Berowne
