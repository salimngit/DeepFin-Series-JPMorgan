{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://camo.githubusercontent.com/290ee18049382f170e5f63956a389da081a8c16f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f636f6d2e726176656e7061636b2e636d732f70616765732f6a702d6d6f7267616e2d6269672d646174612d61692d35343637413139302e6a7067\"  style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"code","source":["import requests\nimport os\nfrom bs4 import BeautifulSoup\nfrom azure.storage.blob import BlockBlobService"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["wasb_account_key = 'TODO'\nwasb_account_name = 'TODO'\n\n# Create the BlockBlockService that is used to call the Blob service for the storage account\nblock_blob_service = BlockBlobService(account_name=wasb_account_name, account_key=wasb_account_key)\n\n# Create a container called 'quickstartblobs'.\ncontainer_name ='deepfin3'\nif block_blob_service.exists(container_name) is False:\n  block_blob_service.create_container(container_name)      "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def scrape_find_zips(url):\n    \"\"\" scrape the given url for any zip files in href tags \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1',\n    }\n\n    response = requests.get(url, headers=headers)\n    #print(response.status_code)\n    #print(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    all_tags = soup.find_all('a', href=True)\n    tag_dict = {}\n    for tag in all_tags:\n      if '.zip' in tag['href']:\n          tag_dict[tag.get_text()] = tag['href']\n    return tag_dict\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["url = \"https://www.asxhistoricaldata.com/archive/\"\n\ntag_dict = scrape_find_zips(url)\n\nprint(tag_dict.values())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from azure.storage.file import ContentSettings\n\ndef scrape_downloader(urls, dest):\n    \"\"\" download all files listed in urls {} dict \n    Input:\n        url = base url\n        urls= target paths to append to base url\n        dest= local path to store files\n    \"\"\"\n    for k_name, v_path in urls.items():\n        src = v_path        \n        \n        headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1',\n        }\n        r = requests.get(src, headers=headers)  \n        if r is not None:\n          block_blob_service.create_blob_from_bytes(dest, k_name + '.zip', r.content)            \n          print(src)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["scrape_downloader(tag_dict, container_name)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["blobs = block_blob_service.list_blobs(container_name)\n\nfor blob in blobs:\n  print(blob.name)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import zipfile\nimport csv\nimport datetime\nfrom io import BytesIO, TextIOWrapper\nfrom pyspark.sql import Row\n\n\n#get all files in container\nf_zip = [ blob.name for blob in blobs]\n\n#Create empty lists\nnewlines = lines = header = all_lines = []\n\n#limit to single \nfor f in f_zip: #[:1]: limit\n    #only go through zip files in container \n    if f.endswith(\".zip\"):\n      f_bytes = block_blob_service.get_blob_to_bytes(container_name, f)\n      with zipfile.ZipFile(BytesIO(f_bytes.content)) as main_zip:  \n        for inner_file in main_zip.namelist():   \n            #print(inner_file)\n            #read inner zip file\n            zfiledata = BytesIO(main_zip.read(inner_file))\n            inner_csv = TextIOWrapper(zfiledata, encoding='utf8', newline='')            \n            reader = csv.reader(inner_csv, delimiter=',', dialect=csv.unix_dialect)\n            \n            for row, line in enumerate(reader):\n                #print(line)\n                # 'Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume'\n                lines.append(Row(Ticker=line[0], Date=datetime.datetime.strptime(line[1], '%Y%m%d') , High=float(line[2]), Low=float(line[3]), Close=float(line[4]), Volume=float(line[5])))\n            "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df = sqlContext.createDataFrame(sc.parallelize(lines))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql drop table if exists asxhistoric;"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df.write.saveAsTable(\"asxhistoric\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.functions import year\n\nfocus_df = ((df \n  .select(\"Date\", \"Close\", \"Ticker\") \n  .filter(\"Ticker = 'AAA'\")\n  .filter(year(\"Date\") == '1998')\n))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["focus_df.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(focus_df)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql drop table if exists timeseries;"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["focus_df.write.saveAsTable(\"timeseries\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import year\n\ndf_2016 = ((df \n  .filter(year(\"Date\") == '2016')\n))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%sql drop table if exists asxhistoric2016;"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_2016.write.saveAsTable(\"asxhistoric2016\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM asxhistoric2016;"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Ingest","notebookId":1521445780264667},"nbformat":4,"nbformat_minor":0}
